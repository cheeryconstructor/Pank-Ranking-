# -*- coding: utf-8 -*-
"""PI_2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PgBMkH6I9lsOsdIlHduvQLhnOF8sP1SZ
"""

! pip3 install stopwords
! pip3 install stop_words

import nltk
nltk.download('popular')

!pip install paramiko

# -*- coding: utf-8 -*-
#import cPickle as pickle
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.datasets import fetch_20newsgroups
from sklearn.decomposition import NMF, LatentDirichletAllocation
from nltk.stem import PorterStemmer, WordNetLemmatizer
import re
from nltk.tokenize import RegexpTokenizer
tokenizer = RegexpTokenizer(r'\w+')
from nltk.corpus import stopwords
stop = stopwords.words('english')
wnl = WordNetLemmatizer()
from nltk.tokenize import RegexpTokenizer
from stop_words import get_stop_words
from nltk.stem.porter import PorterStemmer
from gensim import corpora, models
import gensim
import numpy as np

tokenizer = RegexpTokenizer(r'\w+')

# create English stop words list
en_stop = get_stop_words('en')
#print en_stop
# Create p_stemmer of class PorterStemmer

import pandas as pd

df=pd.read_csv("article.csv")


list1=list(df["articleNo"])
#print len(list1)     ##this list1 contains all such articles

f=open("article_text.txt","w")
list2=list(df["text"])
for i in list2:
	f.write(i)
	f.write("******************************")	

p_stemmer = PorterStemmer()
with open("article_text.txt","r") as p:
	documents= ((p.read()).split("******************************"))
texts = []

# loop through document list
for i in documents:
    
    # clean and tokenize document string
    raw = i.lower()
    text1=(''.join([i if ord(i) >32 and ord(i) < 128 else ' ' for i in raw]))
    result = ''.join([i for i in text1 if not i.isdigit()])
    tokens = tokenizer.tokenize(result)

    # remove stop words from tokens
    stopped_tokens = [i for i in tokens if not i in en_stop]
    just_more = [j for j in stopped_tokens if not j in ["said","go","s","getty","will","one", "also","can","t","u",'st']]
    
    # stem tokens
    #stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]
    
    # add tokens to list
    texts.append(just_more)

# turn our tokenized documents into a id <-> term dictionary
dictionary = corpora.Dictionary(texts)
    
# convert tokenized documents into a document-term matrix
corpus = [dictionary.doc2bow(text) for text in texts]

# generate LDA model
ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=5, id2word = dictionary, passes=10)



news=ldamodel.print_topics(num_topics=5, num_words=10)

print(news[19])
print(news)

#print (len(string))
new = dict()
for (id,str) in  news:
  new[id]= [sub.split('*') for sub in str.split(' + ')]

from pprint import pprint
for key in new.keys():
  new[key] = [[float(probab),keyword.strip('"')] for probab, keyword in new[key]]
pprint(new)

#print(new[5][0][0])

G = [[key1, key2] for key1 in new.keys() for key2 in new.keys() if key1 != key2]
pprint(G)

print(new.keys())

keys = list(new.keys())

A = np.zeros((len(keys),len(keys)))

#applying Algorithm for finding closeness

#d=0
for i in range(len(keys)):
  for j in range(len(keys)):
    for x in range(len(new[keys[i]])):
      for y in range(len(new[keys[j]])):
        #d =  d + new[keys[i]][x][0]+new[keys[j]][y][0]
        if new[keys[i]][x][1]== new[keys[j]][y][1]:
          #A[i][j] += new[keys[i]][x][0]
          A[i][j]=A[i][j] + new[keys[i]][x][0]/(new[keys[i]][x][0]+new[keys[i]][y][0])
        #A[i][j]= A[i][j]/d 
      d=0

print(A)

#Normalising The Closeness Matrix

for i in range(len(keys)):
  sum=0
  for j in range(len(keys)):
    sum=sum+A[i][j]
    
  for j in range(len(keys)):
    A[i][j]=A[i][j]/sum

print(A)

#Appying Damping on Matrix

for i in range(len(keys)):
  for j in range(len(keys)):

    A[i][j]=0.8*A[i][j] + 0.2*(1/len(keys))

print(A)





#Finding Rank Vector using Itterations

R=0.25*np.ones((4,1))
R=np.transpose(R)
for i in range(100):
  R=R.dot(A)
  print(R)

print(A.shape,R.shape)

